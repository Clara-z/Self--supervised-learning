{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Weights loaded.\n",
      "[[  2.45648175e-02   7.99257401e-03  -2.18111295e-02 ...,   1.63101871e-02\n",
      "   -1.44028785e-02  -1.49583386e-03]\n",
      " [ -1.31063275e-02  -2.12847479e-02   1.12108001e-03 ...,  -2.07275455e-03\n",
      "    2.81009451e-03   2.07442325e-02]\n",
      " [ -3.03017162e-03   1.08668329e-02   7.30061950e-03 ...,   1.21660037e-02\n",
      "   -1.80090684e-02  -2.37702504e-02]\n",
      " ..., \n",
      " [ -1.23752560e-02  -3.11842444e-03  -5.39299054e-03 ...,  -4.30203881e-03\n",
      "   -6.89352091e-05   1.83842462e-02]\n",
      " [ -2.26896722e-02  -1.37367304e-02  -1.84576660e-02 ...,   2.15333886e-02\n",
      "   -5.92074776e-03  -8.16717930e-03]\n",
      " [  6.56852545e-03  -7.13132648e-03   3.10798245e-03 ...,   5.34924166e-03\n",
      "    4.51841438e-03   1.36570046e-02]]\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 49 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 50 / 50\n",
      "[t-SNE] Mean sigma: 0.068787\n",
      "[t-SNE] Iteration 25: error = 1.4110159, gradient norm = 0.0022629\n",
      "[t-SNE] Iteration 50: error = 1.3349496, gradient norm = 0.0017485\n",
      "[t-SNE] Iteration 75: error = 1.2308381, gradient norm = 0.0008351\n",
      "[t-SNE] Iteration 75: gradient norm 0.000835. Finished.\n",
      "[t-SNE] Error after 75 iterations with early exaggeration: 1.230838\n",
      "[t-SNE] Iteration 100: error = 1.1968672, gradient norm = 0.0005886\n",
      "[t-SNE] Iteration 100: gradient norm 0.000589. Finished.\n",
      "[t-SNE] Error after 100 iterations: 1.196867\n",
      "('vis_data  vector has shape', (50, 2))\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "#loading the weights from the saved file but for that we have to compile the model\n",
    "#Compiling the model\n",
    "vocabsize=9400\n",
    "vector_embedding = 128\n",
    "maxlen = 4\n",
    "fixlen=3\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_dim=vocabsize+1, output_dim=vector_embedding, input_length=fixlen, return_sequences=True))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(input_dim=vector_embedding, output_dim=vector_embedding, input_length=fixlen, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabsize+1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#Loading the weights\n",
    "directory ='/home/sinah/'\n",
    "weights_path= directory + 'model_weight.hdf5'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k ==0:\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "        x=model.layers[k].get_weights()\n",
    "        embedding = np.array(x[0])\n",
    "    else:\n",
    "        break\n",
    "f.close()\n",
    "print('Embedding Weights loaded.')\n",
    "\n",
    "#Sigmoid transformation of weights and then to binary\n",
    "top_words_count=50\n",
    "embedding = 1/(1+np.exp(embedding*-1))\n",
    "embedding_capped_with_max_words=np.zeros((top_words_count,len(embedding[0])))\n",
    "for rows in range(0,top_words_count):\n",
    "    for cols in range(0,len(embedding[0])):\n",
    "        if embedding[rows][cols]<.5:\n",
    "        embedding_capped_with_max_words[rows][cols] = embedding[rows][cols]\n",
    "        else:\n",
    "            embedding_capped_with_max_words[rows][cols] = 1 \n",
    "\n",
    "#Get the top 100 words in vocab  in word list\n",
    "counter=0\n",
    "text_open=open(directory + 'vocab.txt','r')\n",
    "words=[]\n",
    "for item in text_open:\n",
    "    if counter < top_words_count:\n",
    "        words.append(item.strip('\\n'))\n",
    "        counter = counter +1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Get TSNE representation of the words\n",
    "model_tsne = TSNE(n_components=2, verbose=2, n_iter=200)\n",
    "vis_data = model_tsne.fit_transform(embedding_capped_with_max_words)\n",
    "print('vis_data  vector has shape', vis_data.shape)\n",
    "# plot the result\n",
    "vis_x = vis_data[:, 0]\n",
    "vis_y = vis_data[:, 1]\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(vis_x, vis_y)\n",
    "for i, txt in enumerate(words):\n",
    "    ax.annotate(txt, (vis_x[i],vis_y[i]))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
