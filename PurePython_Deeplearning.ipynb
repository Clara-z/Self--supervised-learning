{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Network...\n",
      "|--- DenseLayer\n",
      "    |--- dimensions: (784, 100)\n",
      "|--- ActivationLayer\n",
      "    |--- dimensions: (100, 100)\n",
      "|--- DenseLayer\n",
      "    |--- dimensions: (100, 10)\n",
      "|--- ActivationLayer\n",
      "    |--- dimensions: (10, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinah/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:417: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for epoch 0: 0.8747\n",
      "Accuracy for epoch 1: 0.925\n",
      "Accuracy for epoch 2: 0.9382\n",
      "Accuracy for epoch 3: 0.9257\n",
      "Accuracy for epoch 4: 0.9373\n",
      "Accuracy for epoch 5: 0.9495\n",
      "Accuracy for epoch 6: 0.9453\n",
      "Accuracy for epoch 7: 0.9484\n",
      "Accuracy for epoch 8: 0.949\n",
      "Accuracy for epoch 9: 0.9485\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sigmoid function on doubles and its vectorized version\n",
    "def sigmoid_double(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "sigmoid = np.vectorize(sigmoid_double)\n",
    "\n",
    "\n",
    "# Derivative of sigmoid for doubles and numpy arrays\n",
    "def sigmoid_prime_double(z):\n",
    "    #\n",
    "    # Return the correct result for the derivative of the sigmoid function.\n",
    "    #\n",
    "    # derivative of sigmoid\n",
    "    return (sigmoid(z) * (1.0 - sigmoid(z)))\n",
    "\n",
    "sigmoid_prime = np.vectorize(sigmoid_prime_double)\n",
    "\n",
    "def softmax(w):\n",
    "    e = np.exp(w - np.amax(w))\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def softmax_prime(w):\n",
    "    res = np.zeros((w.shape[0],w.shape[0]))\n",
    "    for rows in range(0,len(w)):\n",
    "        for cols in range(0,len(w)):\n",
    "            if rows == cols:\n",
    "                res[rows][cols] = softmax(w)[rows]*(1- softmax(w)[rows])\n",
    "            else:\n",
    "                res[rows][cols] = -softmax(w)[rows]*softmax(w)[cols]\n",
    "    return res\n",
    "\n",
    "\n",
    "class Objective(object):\n",
    "\n",
    "    def cost_function(self, predictions, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def cost_derivative(self, predictions, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MSE(Objective):\n",
    "    '''\n",
    "    Mean squared error function and its derivative.\n",
    "    '''\n",
    "    def cost_function(self, predictions, labels):\n",
    "        diff = predictions - labels\n",
    "        return 0.5 * sum(diff*diff)[0]\n",
    "\n",
    "    def cost_derivative(self, predictions, labels, typ):\n",
    "        #\n",
    "        # \n",
    "        # the derivative of the above MSE cost function.\n",
    "        # \n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.reshape(predictions.shape[0], predictions.shape[1])\n",
    "        if predictions.shape != labels.shape:\n",
    "            print('Shape Mismatch between Predictions and Labels')\n",
    "            return \n",
    "        else:\n",
    "            return (predictions - labels)\n",
    "class Crossentropy(Objective):\n",
    "    '''\n",
    "    Crossentropy error function and its derivative.\n",
    "    '''\n",
    "    def cost_function(self, predictions, labels):\n",
    "        -sum(labels * np.log(predictions))\n",
    "\n",
    "    def cost_derivative(self, predictions, labels, typ):\n",
    "        #\n",
    "        # the derivative of the above crossentropy cost function.\n",
    "        # \n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.reshape(predictions.shape[0], predictions.shape[1])\n",
    "        if predictions.shape != labels.shape:\n",
    "            print('Shape Mismatch between Predictions and Labels')\n",
    "            return \n",
    "        else:\n",
    "            if typ == 'Softmax':\n",
    "                return (predictions,labels) # Although this derivative is -label/prediction\n",
    "            elif typ == 'Sigmoid':\n",
    "                return -labels/predictions\n",
    "        \n",
    "class Network():\n",
    "    '''\n",
    "    Neural network class, designed to sequentially stack layers in a simple\n",
    "    fashion.\n",
    "\n",
    "    As objective / cost function we use mean square error (MSE), which is found\n",
    "    in the utils module. All layers should be implemented in the layers module.\n",
    "    The model is trained by mini-batch gradientdescent.\n",
    "\n",
    "    Usage:\n",
    "        - Initialize the network by creating a new instance, e.g.\n",
    "            net = Network()\n",
    "        - Add layers one by one using the 'add' method, e.g.\n",
    "            net.add(layers.ActivationLayer(100))\n",
    "        - Train the model on data by calling the 'train' method:\n",
    "            net.train(data, num_of_epochs, mini_batch_size,\n",
    "                      learning_rate, test_data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, objective=None):\n",
    "        print(\"Initialize Network...\")\n",
    "        self.layers = []\n",
    "        if objective is None:\n",
    "            self.objective = Crossentropy()\n",
    "\n",
    "    def add(self, layer):\n",
    "        '''\n",
    "        Add a layer, connect it to its predecessor and let it describe itself.\n",
    "            - layer: A layer from the layers module\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "        layer.describe()\n",
    "        if len(self.layers) > 1:\n",
    "            self.layers[-1].connect(self.layers[-2])\n",
    "\n",
    "    def train(self, training_data, epochs, mini_batch_size,\n",
    "              learning_rate, test_data=None):\n",
    "        '''\n",
    "            First, shuffle training data, then split it into mini batches.\n",
    "            Next, for each mini-batch,\n",
    "            train this batch. In case test data is present, evaluate it.\n",
    "\n",
    "            - training_data: A numpy array of pairs containing features and\n",
    "              labels\n",
    "            - epochs: The number of epochs/iterations to be trained.\n",
    "            - mini_batch_size: Number of samples to fit in one batch\n",
    "            - learning_rate: The learning rate for the gradient descent update\n",
    "              rule\n",
    "            - test_data: Optional test data for evaluation\n",
    "        '''\n",
    "        n_train = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + mini_batch_size]\n",
    "                            for k in range(0, n_train, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.train_batch(mini_batch, learning_rate)\n",
    "            if test_data:\n",
    "                n_test = len(test_data)\n",
    "                print(\"Accuracy for epoch {0}: {1}\".\n",
    "                      format(j, self.evaluate(test_data) / n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def train_batch(self, mini_batch, learning_rate):\n",
    "        '''\n",
    "        Feed forward and pass backward a mini-batch, then update parameters\n",
    "        accordingly.\n",
    "        '''\n",
    "        self.forward_backward(mini_batch)\n",
    "        self.update(mini_batch, learning_rate)\n",
    "\n",
    "    def update(self, mini_batch, learning_rate):\n",
    "        '''\n",
    "        Normalize learning rate, then update each layer.\n",
    "        Afterwards, clear all deltas to start the next batch properly.\n",
    "        '''\n",
    "        learning_rate = learning_rate / len(mini_batch)\n",
    "        for layer in self.layers:\n",
    "            layer.updateParams(learning_rate)\n",
    "        for layer in self.layers:\n",
    "            layer.clearDeltas()\n",
    "\n",
    "    def forward_backward(self, mini_batch):\n",
    "        '''\n",
    "        For each sample in the mini batch, feed the features forward layer by\n",
    "        layer.\n",
    "        Then compute the cost derivative and do layer by layer backpropagation.\n",
    "        '''\n",
    "        for x, y in mini_batch:\n",
    "            self.layers[0].input_data = x\n",
    "            for layer in self.layers:\n",
    "                layer.forward()      \n",
    "            self.layers[-1].input_delta = self.objective.cost_derivative(\n",
    "            self.layers[-1].output_data, y,self.layers[-1].typ)\n",
    "            for layer in reversed(self.layers):\n",
    "                layer.backward()\n",
    "            # This is the derivative testing code and should be only activated when asked\n",
    "            '''\n",
    "            for layer in self.layers:\n",
    "                if layer.name == 'Dense':\n",
    "                    tmp_W=layer.params[0][0][0]\n",
    "                    layer.params[0][0][0]= tmp_W+ 1e-4\n",
    "                    first_der=self.objective.cost_function(self.single_forward(x),y)\n",
    "                    layer.params[0][0][0]= tmp_W - 1e-4\n",
    "                    sec_der=self.objective.cost_function(self.single_forward(x),y)\n",
    "                    print('The grad check for Dense layers =',(first_der - sec_der)/(2*1e-4))\n",
    "                    print('The actual derivatives=',np.dot(layer.get_backward_input(),layer.get_forward_input().T)[0][0])\n",
    "                    layer.params[0][0][0] = tmp_W\n",
    "            '''\n",
    "                    \n",
    "\n",
    "    def single_forward(self, x):\n",
    "        '''\n",
    "        Pass a single sample forward and return the result.\n",
    "        '''\n",
    "        self.layers[0].input_data = x\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "        return self.layers[-1].output_data\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        '''\n",
    "        Returns the number of correctly predicted labels.\n",
    "        '''\n",
    "        #\n",
    "        # For each sample in test data compute the outcome produced by the\n",
    "        # neural net by using 'single_forward'. Then determine the index of the\n",
    "        # largest element in the outcome vector to compare it to the respective\n",
    "        # label.\n",
    "        #\n",
    "        # For instance, if output_data = (0.0, 0.9, 0.1), then 0.9 is the most\n",
    "        # likely result of the network and its position in the array should\n",
    "        # match the label.\n",
    "        #\n",
    "        # Return the number of correctly predicted labels.\n",
    "        count=0\n",
    "        for x,y in test_data:\n",
    "            output = self.single_forward(x)\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                count =  count +1\n",
    "        return count\n",
    "    \n",
    "class Layer(object):\n",
    "    '''\n",
    "    A layer in a neural network. A layer knows its predecessor ('previous')\n",
    "    and its successor ('next'). Each layer has a forward function that\n",
    "    emits output data from input data and a backward function\n",
    "    that emits an output delta, i.e. a gradient, from an input delta.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "        self.previous = None\n",
    "        self.next = None\n",
    "\n",
    "        self.output_data = None\n",
    "        self.output_delta = None\n",
    "\n",
    "        self.input_data = None\n",
    "        self.input_delta = None\n",
    "        \n",
    "        self.name= None\n",
    "        \n",
    "        self.typ = None\n",
    "\n",
    "    def connect(self, layer):\n",
    "        '''\n",
    "        Connect a layer to its neighbours.\n",
    "        '''\n",
    "        self.previous = layer\n",
    "        layer.next = self\n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Feed input data forward. Start with:\n",
    "            data = self.get_forward_input()\n",
    "        to receive the output of the previous layer.\n",
    "        The last line should set the output of the layer, i.e. look like\n",
    "        this:\n",
    "            self.output_data = output_data_of_this_layer\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_forward_input(self): \n",
    "        '''\n",
    "        input_data is reserved for the first layer, all others get their\n",
    "        input from the previous output.\n",
    "        '''\n",
    "        if self.previous is not None:\n",
    "            return self.previous.output_data\n",
    "        else:\n",
    "            return self.input_data\n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        Similar to the forward pass compute backpropagation of error terms,\n",
    "        i.e. feed input errors backward by starting with:\n",
    "            delta = self.get_backward_input()\n",
    "        At the end, set the error term of this layer like this:\n",
    "            self.output_delta = output_error_of_this_layer\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_backward_input(self):\n",
    "        '''\n",
    "        Input delta is reserved for the very last layer, which will be set\n",
    "        to the derivative of the cost function. All other layers get their\n",
    "        error terms from their successor.\n",
    "        '''\n",
    "        if self.next is not None:\n",
    "            return self.next.output_delta\n",
    "        else:\n",
    "            return self.input_delta\n",
    "    \n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    '''\n",
    "    In general, an activation layer computes the activation of neurons in the\n",
    "    network with some activation function like sigmoid, tanh, etc.\n",
    "    Here we simply use the sigmoid function for simplicity.\n",
    "    '''\n",
    "    def __init__(self, input_dim, typ):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "        self.name ='Activation'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim\n",
    "        self.typ = typ\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        if self.typ == 'Sigmoid':\n",
    "            self.output_data = sigmoid(data)\n",
    "        elif self.typ == 'Softmax':\n",
    "            self.output_data = softmax(data)\n",
    "            \n",
    "    def backward(self):\n",
    "        delta = self.get_backward_input()\n",
    "        data = self.get_forward_input()\n",
    "        if self.typ == 'Sigmoid':\n",
    "            self.output_delta = delta * sigmoid_prime(data)\n",
    "        elif self.typ == 'Softmax':\n",
    "            self.output_delta =  delta[0] - delta[1]\n",
    "        \n",
    "    def clearDeltas(self):\n",
    "        pass\n",
    "\n",
    "    def updateParams(self, rate):\n",
    "        pass\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|--- \" + self.__class__.__name__)\n",
    "        print(\"    |--- dimensions: ({0}, {1})\"\n",
    "              .format(str(self.input_dim), str(self.output_dim)))\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    '''\n",
    "    Classic feed-forward layer. Output is defined as W * x + b.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super(DenseLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.name ='Dense'\n",
    "\n",
    "        self.weight = np.random.randn(output_dim, input_dim)\n",
    "        self.bias = np.random.randn(output_dim, 1)\n",
    "        self.params = [self.weight, self.bias]\n",
    "\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        #\n",
    "        # the forward computation of the dense layer as sketched below.\n",
    "        self.output_data = np.zeros((self.output_dim, 1))\n",
    "        self.output_data = np.dot(self.weight,data) + self.bias\n",
    "      \n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        delta = self.get_backward_input()\n",
    "        data = self.get_forward_input()\n",
    "        self.delta_b += delta\n",
    "        #\n",
    "        # Calculating the loss der wrt to weights in terms of error from the previous layer (called delta) \n",
    "        self.delta_w += np.dot(delta,data.T) #  + .001* self.weight # Adding a regulariser\n",
    "        #\n",
    "        # the back propagation step by computing the gradient to be\n",
    "        # passed to the previous layer. \n",
    "        self.output_delta = np.zeros((self.input_dim, 1))\n",
    "        self.output_delta = np.dot(self.weight.T,delta)\n",
    "\n",
    "    def clearDeltas(self):\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def updateParams(self, rate):\n",
    "        self.weight -= rate * self.delta_w\n",
    "        self.bias -= rate * self.delta_b\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|--- \" + self.__class__.__name__)\n",
    "        print(\"    |--- dimensions: ({0}, {1})\"\n",
    "              .format(str(self.input_dim), str(self.output_dim)))\n",
    "\n",
    "# Initialize a network and call it 'net'. Add four layers to it as follows.\n",
    "# First layer: Dense layer with input dimension 784 and output dimension 100\n",
    "# Second layer: Activation layer of dimension 100\n",
    "# Third layer: Dense layer with input dimension 100 and output dimension 10\n",
    "# Fourth layer: Activation layer of dimension 10\n",
    "\n",
    "net = Network()\n",
    "net.add(DenseLayer(784,100))\n",
    "net.add(ActivationLayer(100,'Sigmoid'))\n",
    "net.add(DenseLayer(100,10))\n",
    "net.add(ActivationLayer(10,'Softmax'))\n",
    "\n",
    "# Load train and test data using 'load_mnist'\n",
    "\n",
    "def encode_label(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "def shape_data(data, encode=True):\n",
    "    features = [np.reshape(x, (784, 1)) for x in data[0]]\n",
    "    if encode:\n",
    "        labels = [encode_label(y) for y in data[1]]\n",
    "    else:\n",
    "        labels = data[1]\n",
    "    return list(zip(features, labels))\n",
    "\n",
    "def from_csv_cols(data, num_features=784):\n",
    "    features = data[:, :num_features]\n",
    "    labels = data[:, num_features].flatten()\n",
    "    return (features, labels)\n",
    "\n",
    "def load_csv_data(train_idx=50000):\n",
    "    data = pd.read_csv('data.csv', sep=',', header=None)\n",
    "    np_data = data.values\n",
    "    raw_train_data = from_csv_cols(np_data[:train_idx, ])\n",
    "    raw_test_data = from_csv_cols(np_data[train_idx:, ])\n",
    "    train_data = shape_data(raw_train_data)\n",
    "    test_data = shape_data(raw_test_data)\n",
    "    return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_csv_data()\n",
    "#\n",
    "# Train the network with train and test data you loaded before. Train 10\n",
    "# epochs, using a mini-batch size of 10 and a learning rate of 3.0\n",
    "#\n",
    "net.train(train_data, epochs=10, mini_batch_size=10,learning_rate=3, test_data=test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
